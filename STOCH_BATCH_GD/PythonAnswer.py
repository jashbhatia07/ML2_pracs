# -*- coding: utf-8 -*-
"""ML2_Stochastic_&_Batch_GD.ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16TKh5weZz3zBU_sZFRyIigvDs2JmjSNk
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math

def func(X,w,b):
  return np.dot(X,w) + b

def sig(yin):
  return 1/(1+ math.exp(-yin))

def mse(yhat,y):
  mse=0
  # print(yhat,y)
  for i in range(len(y)):
    mse+=(yhat[i]-y[i])**2
  return mse/2

def plot_mse(mse_list,epochs):
  x=[i for i in range(epochs)]
  plt.plot(x,mse_list)

"""##Stochastic Gradient Descent"""

def stochastic_gd (X,Y):
  w,b,c,epoch = -2,-2,1,300
  mse_list=[]
  for i in range(epoch):
    result = []
    for x,y in zip(X,Y):
      yhat = sig(func(x,w,b))
      delw = c*(y-yhat)*yhat*(1-yhat)*x
      delb = c*(y-yhat)*yhat*(1-yhat)
      w += delw
      b += delb
      result.append(yhat)
      if (len(result)==len(Y)):
        mse_list.append(mse(result,Y))
    print(f"Epoch: {i+1}\tWeight: {w}, Bias: {b}\t y_hat: {result}\t MSE: {mse_list[-1]}")
  plot_mse(mse_list,epoch)
  print(f"\n\nFinal weights: {w}\tBias {b}\t\tMean Squared Error: {mse_list[-1]}")

"""Taking the dataset with initial value of X = [0.5, 2.5], Y= [0.2, 0.9] 

Initializing a neural network with random weights - weight = -2, bias= -2 and constant (c) = 1.

"""

X = [0.5,2.5]
Y = [0.2,0.9]
stochastic_gd(X,Y)

"""For each of the 300 epochs, we 
calculate the mean squared error, 
note the weight and bias changes.

Finally, we also plot the graph of MSE against the epoch and we get

 Final weights: 1.7374342738186377	Bias -2.202814158199366		Mean Squared Error: 5.223759084367854e-05 using Stochastic Gradient Descent.

##Batch Gradient Descent
"""

def batch_gd (X,Y):
  w,b,c,epoch = -2,-2,1,300
  mse_list=[]
  for i in range(epoch):
    result = []
    delw=delb=0
    for x,y in zip(X,Y):
      yhat = sig(func(x,w,b))
      delw += c*(y-yhat)*yhat*(1-yhat)*x
      delb += c*(y-yhat)*yhat*(1-yhat)
      result.append(yhat)
      if (len(result)==len(Y)):
        mse_list.append(mse(result,Y))
    delw = delw/len(Y)
    delb = delb/len(Y)
    w += delw
    b += delb
    print(f"Epoch: {i+1}\tWeight: {w}, Bias: {b}\t y_hat: {result}\t MSE: {mse_list[-1]}")
  plot_mse(mse_list,epoch)
  print(f"\n\nFinal weights: {w}\tBias {b}\tMean Squared Error: {mse_list[-1]}")

X = [0.5,2.5]
Y = [0.2,0.9]
batch_gd(X,Y)

"""For each of the 300 epochs, we 
calculate the mean squared error, 
note the weight and bias changes.

Finally, we also plot the graph of MSE against the epoch and we get

Final weights: 1.4418720872208577	Bias -1.7602101517624822	Mean Squared Error: 0.002585845364868681 using Batch Gradient Descent.
"""